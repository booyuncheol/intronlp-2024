# 세션 2 - 딥 러닝의 진화와 고급 신경망 아키텍처

이 세션에서는 기초적인 신경망 모델에서 현대의 딥 러닝 아키텍처로의 흥미로운 여정을 탐험합니다. 우리는 제프리 힌튼의 중요한 공헌, 합성곱 신경망(CNNs)의 발전, 그리고 혁신적인 트랜스포머 모델의 도입에 초점을 맞출 것입니다. 이러한 진화는 인공지능을 혁명적으로 변화시켜, 기계가 한때는 불가능하다고 여겨졌던 방식으로 복잡한 데이터를 처리하고 이해할 수 있게 만들었습니다.

## 딥 러닝의 부상

### 깊은 네트워크 훈련의 도전과제

1990년대와 2000년대 초, 연구자들은 깊은 신경망(DNNs)을 훈련시키는 데 상당한 어려움을 겪었습니다. 가장 두드러진 문제 중 하나는 **기울기 소실 문제**였습니다.

#### 기울기 소실 문제 설명

긴 사람 체인을 통해 메시지를 전달하려고 한다고 상상해 보세요. 메시지가 전달되면서 왜곡되고, 끝에 도달할 때쯤이면 알아볼 수 없게 될 수 있습니다. 마찬가지로, 깊은 신경망에서 더 많은 층을 추가할수록, 가중치를 업데이트하는 데 사용되는 기울기(우리의 "메시지"와 같은)가 네트워크를 통해 역방향으로 전파될 때 극도로 작아지거나(또는 "소실") 됩니다. 이로 인해 초기 층들이 효과적으로 학습하기 어려워집니다. 의미 있는 피드백을 거의 받지 못하기 때문입니다.

### 제한 볼츠만 머신(RBMs)을 통한 돌파구

2006년, 제프리 힌튼은 **제한 볼츠만 머신(RBMs)**과 **계층별 사전 훈련**을 사용하여 이러한 도전과제를 해결하는 새로운 접근법을 소개했습니다.

#### 계층별 사전 훈련

이 기술은 입력 층에서 시작하여 출력 층으로 이동하면서 네트워크를 한 층씩 훈련시키는 것을 포함합니다. 각 층은 초기에 RBM으로 훈련되어 입력을 재구성하는 법을 학습합니다. 이 사전 훈련 단계 후, 전체 네트워크는 역전파를 사용하여 미세 조정됩니다.

이것은 마치 고층 건물을 짓는 것과 같습니다: 모든 층을 동시에 건설하려고 하지 않고, 다음 층으로 이동하기 전에 각 층을 건설하고 안정화시킵니다. 이 접근법은 네트워크의 가중치를 더 나은 상태로 초기화하는 데 도움을 주어, 후속 미세 조정 과정을 더 효과적으로 만듭니다.

## 제프리 힌튼의 신경망에 대한 주요 공헌

종종 "딥 러닝의 대부"로 불리는 제프리 힌튼은 신경망과 인공지능 분야를 형성한 여러 혁신적인 공헌을 했습니다.

### 1. 역전파 (1986)

1986년, 힌튼은 데이비드 루멜하트와 로널드 윌리엄스와 함께 다층 신경망을 훈련시키기 위한 **역전파 알고리즘**을 소개했습니다.

![역전파 이미지](figs/Hinton-papers-1.jpg)
![역전파 이미지](figs/Hinton-papers-2.jpg)

#### 역전파의 작동 방식

복잡한 투석기의 조준을 조정하여 목표물을 맞추려고 한다고 상상해 보세요. 발사를 하고, 얼마나 빗나갔는지 확인한 다음, 그에 따라 기계의 각 부분을 조정합니다. 역전파도 비슷하게 작동합니다:

1. 네트워크가 예측을 합니다.
2. 오차(예측과 실제 출력 간의 차이)가 계산됩니다.
3. 이 오차는 네트워크를 통해 역방향으로 전파됩니다.
4. 각 가중치는 오차에 대한 기여도에 따라 조정됩니다.

이 과정을 통해 네트워크는 자신의 실수로부터 학습하고 점진적으로 성능을 향상시킬 수 있습니다. 마치 각 시도마다 투석기를 미세 조정하는 것과 비슷합니다.

### 2. 깊은 신뢰 신경망 (DBNs) (2006)

2006년, 힌튼은 기울기 소실 문제를 극복하기 위해 계층별 사전 훈련을 사용하는 **깊은 신뢰 신경망(DBNs)**을 소개했습니다.

![깊은 신뢰 신경망 이미지](figs/Hinton-papers-3.jpg)

#### DBNs 이해하기

DBNs를 각 블록이 RBM인 빌딩 블록 타워로 생각해 보세요. 네트워크는 아래에서 위로 구축되며, 각 층은 아래 층에서 받은 데이터를 표현하는 법을 학습합니다. 이 접근법을 통해 네트워크는 상위 층으로 올라갈수록 점점 더 추상적인 특징을 학습할 수 있습니다.

예를 들어, 이미지 인식에서:

- 첫 번째 층은 가장자리를 감지하는 법을 배울 수 있습니다.
- 두 번째 층은 이러한 가장자리를 조합하여 간단한 형태를 인식할 수 있습니다.
- 더 높은 층은 얼굴이나 물체와 같은 더 복잡한 패턴을 인식할 수 있습니다.

이러한 계층적 학습은 DBNs를 복잡한 패턴 인식 작업에 특히 효과적으로 만듭니다.

### 3. RBMs를 이용한 차원 축소 (2006)

또한 2006년에 힌튼과 루슬란 살라후트디노프는 RBMs를 **차원 축소**에 사용하는 방법을 보여주었습니다.

![차원 축소 이미지](figs/Hinton-papers-4.jpg)

#### 차원 축소 설명

도시의 크고 상세한 지도가 있지만 작은 종이에 표현해야 한다고 상상해 보세요. 가장 중요한 특징만 유지하면서 단순화해야 할 것입니다. 이것이 본질적으로 차원 축소가 데이터에 대해 하는 일입니다.

RBMs는 고차원 데이터(예: 이미지)를 가장 중요한 특징을 포착하는 저차원 표현으로 압축하는 법을 학습할 수 있습니다. 이는 후속 처리를 더 효율적으로 만들고 데이터 압축, 특징 추출, 심지어 새로운 데이터 샘플 생성과 같은 작업에 도움이 될 수 있습니다.

### 4. AlexNet (2012)

2012년, 힌튼은 학생인 알렉스 크리제프스키와 일야 서츠케버와 함께 ImageNet 대회에서 우승하고 컴퓨터 비전을 혁명화한 합성곱 신경망인 AlexNet을 개발했습니다.

![AlexNet 이미지](figs/Hinton-papers-5.jpg)

#### AlexNet의 영향

AlexNet은 여러 가지 이유로 게임 체인저였습니다:

1. **깊은 아키텍처**: 여러 합성곱 층을 사용하여 이미지에서 복잡한 특징을 학습할 수 있었습니다.
2. **GPU 가속**: GPU 컴퓨팅을 활용하여 이전보다 훨씬 더 큰 네트워크를 훈련시킬 수 있었습니다.
3. **새로운 기술**: ReLU 활성화와 드롭아웃과 같은 기술을 도입했는데, 이는 지금 많은 신경망에서 표준이 되었습니다.

AlexNet이 ImageNet 대회에서 성공(오류율을 26%에서 15.3%로 줄임)한 것은 컴퓨터 비전에서 딥 러닝 혁명의 시작을 알렸습니다.

### 5. 시각적 대조 학습 (2020)

2020년, 힌튼은 시각적 작업을 위한 **대조 학습**의 발전에 기여했습니다.

![시각적 대조 학습 이미지](figs/Hinton-papers-6.jpg)

#### 대조 학습 이해하기

대조 학습은 아이에게 물체를 인식하도록 가르치는 것과 비슷합니다. 이미지 쌍을 보여주고 "이것들이 같은 물체인가요?"라고 묻는 것과 같습니다. 시간이 지남에 따라 아이(또는 이 경우 신경망)는 다른 물체를 구별하는 핵심 특징을 식별하는 법을 배웁니다.

기계 학습의 맥락에서:

1. 네트워크에 같은 이미지의 다른 증강(회전이나 색상 변경 등)이 보여집니다.
2. 이러한 증강이 같은 기본 이미지를 나타낸다는 것을 인식하는 법을 배웁니다.
3. 이 과정은 네트워크가 라벨이 붙은 데이터 없이도 강건한 시각적 표현을 학습하는 데 도움을 줍니다.

이 접근법은 라벨이 붙은 데이터가 부족하거나 얻기 비싼 시나리오에서 특히 유용했습니다.

## 합성곱 신경망 (CNNs)

힌튼의 연구가 딥 러닝을 부활시킨 반면, 얀 르쿤과 다른 연구자들은 **합성곱 신경망(CNNs)**이라 불리는 특별한 종류의 신경망을 발전시켰습니다.

### CNNs의 작동 방식

큰 벽화를 보고 있다고 상상해 보세요. 전체 이미지를 한 번에 받아들이지 않고, 눈이 이미지를 가로질러 스캔하면서 다른 부분에 초점을 맞춥니다. CNNs도 비슷하게 작동합니다:

1. **합성곱 층**: 이는 슬라이딩 윈도우처럼 작동하여 이미지를 스캔하고 가장자리, 질감, 형태와 같은 특징을 감지합니다.
2. **풀링 층**: 특정 영역에서 감지된 특징을 요약하여 네트워크가 위치의 작은 변화에 더 강건해지도록 합니다.
3. **완전 연결 층**: 합성곱 층과 풀링 층이 학습한 고수준 특징을 가져와 최종 분류에 사용합니다.

이 아키텍처는 CNNs를 이미지 분류, 물체 감지, 심지어 얼굴 인식과 같은 작업에 특히 효과적으로 만듭니다.

## 순환 신경망과 LSTMs

텍스트나 시계열과 같은 순차 데이터를 처리하기 위해 연구자들은 **순환 신경망(RNNs)**과 그 더 발전된 변형인 **장단기 메모리(LSTM) 네트워크**를 개발했습니다.

### RNNs와 LSTMs 이해하기

RNN을 기억을 가진 네트워크로 생각해 보세요. 문장과 같은 시퀀스를 처리할 때, 현재 입력뿐만 아니라 이전에 본 것도 고려합니다. 하지만 기본 RNNs는 장기 의존성을 다루는 데 어려움을 겪습니다.

Sepp Hochreiter와 Jürgen Schmidhuber가 도입한 LSTMs는 더 정교한 메모리 셀을 사용하여 이 문제를 해결합니다. 정보를 선택적으로 기억하거나 잊을 수 있어, 장기 의존성을 더 효과적으로 포착할 수 있습니다.

예를 들어, 언어 번역에서 LSTM은 주어가 많은 단어로 동사와 분리되어 있더라도 문장의 주어를 기억할 수 있어, 번역의 문법적 정확성을 보장합니다.

## 트랜스포머 네트워크

2017년, Vaswani 등은 **트랜스포머 아키텍처**를 소개했는데, 이는 많은 최첨단 언어 모델의 근간이 되었습니다.

### 트랜스포머의 작동 방식

트랜스포머는 **자기 주의(self-attention)**라는 메커니즘을 사용하여 각 요소를 처리할 때 입력의 다른 부분의 중요성을 가중치로 부여할 수 있습니다.

파티에서 여러 대화를 따라가려고 하는 상황을 상상해 보세요. 당신에게 관련 있는 것에 기반하여 다른 화자들에게 주의를 기울입니다. 트랜스포머도 비슷하게 작동합니다:

1. 문장의 각 단어에 대해, 트랜스포머는 다른 모든 단어에 얼마나 주의를 기울일지 계산합니다.
2. 이를 통해 단어들 사이의 복잡한 관계를 포착할 수 있으며, 문장 내에서 멀리 떨어져 있더라도 가능합니다.
3. RNNs와 달리, 트랜스포머는 모든 단어를 병렬로 처리할 수 있어 훈련 속도가 훨씬 빠릅니다.

트랜스포머는 기계 번역, 텍스트 생성, 질문 답변과 같은 NLP 작업을 혁신했습니다. BERT와 GPT와 같은 모델들은 트랜스포머 아키텍처를 기반으로 하며, 언어 이해와 생성에 새로운 기준을 세웠습니다.

## 응용 및 미래 방향

신경망 아키텍처의 발전은 다양한 분야에서 획기적인 응용을 이끌어냈습니다:

- **이미지 및 음성 인식**: CNNs는 이미지에서 정확한 물체 감지와 실시간 음성 인식을 가능하게 했습니다.
- **자연어 처리**: 트랜스포머 기반 모델들은 고급 챗봇, 기계 번역 시스템, 심지어 AI 글쓰기 어시스턴트를 구동합니다.
- **헬스케어**: 딥 러닝 모델들은 의료 영상 분석, 신약 발견, 개인화된 의학에 사용되고 있습니다.
- **자율 주행 차량**: CNNs와 다른 딥 러닝 모델들은 자율 주행 차량의 인지와 의사 결정에 중요합니다.

이러한 기술들이 계속 발전함에 따라, 우리는 점점 더 인간과 비슷한 방식으로 세상을 이해하고 상호작용할 수 있는 더욱 정교한 AI 시스템을 보게 될 것입니다.

## 주요 요점

1. **계층별 훈련**: 힌튼이 선구적으로 개발한 이 기술은 깊은 네트워크 훈련의 도전과제를 극복하는 데 중요했습니다.
2. **전문화된 아키텍처**: 시각적 작업을 위한 CNNs와 순차 데이터를 위한 트랜스포머는 AI가 복잡한 실제 정보를 처리하는 능력을 극적으로 향상시켰습니다.
3. **확장성과 효율성**: 트랜스포머와 같은 현대적 아키텍처는 거대한 데이터셋에서 엄청나게 큰 모델을 훈련시키는 것을 가능하게 만들어, AI가 달성할 수 있는 것의 경계를 넓혔습니다.
4. **학제간 영향**: 신경망의 진화는 헬스케어에서 자율 시스템에 이르기까지 수많은 분야에 광범위한 영향을 미쳤습니다.

우리가 이러한 아키텍처를 계속 개선하고 새로운 것을 개발함에 따라, AI의 잠재적 응용 분야는 확장될 것이며, 앞으로 몇 년 동안 흥미진진한 발전을 약속합니다.
